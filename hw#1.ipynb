{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2017121261 / 소프트웨어학과 / 허태영 / HW#1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pytorch lightning Cifar10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch lightning 툴과 Cifar10 dataset을 이용하여 학습하시면 됩니다. model은 자유입니다.\n",
    "\n",
    "실험 결과(Loss, Accuracy) 는 Weight & Bias 를 이용해주세요. \n",
    "\n",
    "\n",
    "\n",
    "제출은 Git 링크와 실험 결과를 로그한 Weight & Bias 링크를 첨부해주면 됩니다.\n",
    "\n",
    "weight&bias 링크의 프로젝트명은 누군지 알아볼 수 있도록 [학번_이름_과제명]형식으로 작성해주세요.\n",
    "\n",
    "\n",
    "\n",
    "[Weight & Bias 참조 링크]\n",
    "\n",
    "https://wandb.ai/wandb_fc/korean/reports/Weights-Biases-Pytorch-Lightning---VmlldzozNzAxOTg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# CIFAR-10\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Pytorch Lightning\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# WandB\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "# model module\n",
    "from src.models.resnet import ResidualBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.13.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.19"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/DeepLearningApplications/2022_2nd_semester_DeepLearning_Applications/wandb/run-20220929_124804-1cm33dma</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/hty/2017121261_TaeyeongHeo_HW1/runs/1cm33dma\" target=\"_blank\">hty</a></strong> to <a href=\"https://wandb.ai/hty/2017121261_TaeyeongHeo_HW1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_logger = WandbLogger(\n",
    "    name='hty',\n",
    "    project='2017121261_TaeyeongHeo_HW1'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR='../data' # cifar-10 data is ignored by .gitignore\n",
    "NUM_CLASSES = 10\n",
    "NUM_WORKERS = 12\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10 DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_transform, data_dir='./', batch_size=32, num_workers=8):\n",
    "        super().__init__()\n",
    "        self.train_transform = train_transform\n",
    "        self.val_transform = transforms.ToTensor()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        datasets.CIFAR10(root=self.data_dir, train=True, download=True)\n",
    "        datasets.CIFAR10(root=self.data_dir, train=False, download=True)\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        if stage == 'fit' or stage is None:\n",
    "            self.train_dset = datasets.CIFAR10(root=self.data_dir, train=True,\n",
    "                                               transform=self.train_transform)\n",
    "            self.val_dset = datasets.CIFAR10(root=self.data_dir, train=False,\n",
    "                                             transform=self.val_transform)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.train_dset, batch_size=self.batch_size,\n",
    "                                           num_workers=self.num_workers, pin_memory=True)\n",
    "\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.val_dset, batch_size=self.batch_size,\n",
    "                                           num_workers=self.num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = CIFAR10DataModule(train_transform,\n",
    "                                data_dir=DATA_DIR,\n",
    "                                batch_size=BATCH_SIZE,\n",
    "                                num_workers=NUM_WORKERS\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data_module.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def init_linear(m):\n",
    "    if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "        if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "\n",
    "def conv_bn(in_channels, out_channels, kernel_size=3, stride=1):\n",
    "    padding = (kernel_size - 1) // 2\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=False),\n",
    "        nn.BatchNorm2d(out_channels)\n",
    "    )\n",
    "\n",
    "def residual_body(in_channels, repetitions, strides):\n",
    "    layers = []\n",
    "    res_channels = in_channels\n",
    "    for rep, stride in zip(repetitions, strides):\n",
    "        for _ in range(rep):\n",
    "            layers.append(ResidualBlock(in_channels, res_channels, stride))\n",
    "            in_channels = res_channels\n",
    "            stride = 1\n",
    "        res_channels = res_channels * 2\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def stem(channel_list, stride):\n",
    "    layers = []\n",
    "    for in_channels, out_channels in zip(channel_list, channel_list[1:]):\n",
    "        layers += [conv_bn(in_channels, out_channels, stride=stride), nn.ReLU(inplace=True)]\n",
    "        stride = 1\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def head(in_channels, classes, p_drop=0.):\n",
    "    return nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.Linear(in_channels, classes)\n",
    "        )\n",
    "\n",
    "def resnet(repetitions, classes, strides=None, p_drop=0.):\n",
    "    if not strides: strides = [2] * (len(repetitions) + 1)\n",
    "    return nn.Sequential(\n",
    "        stem([3, 32, 32, 64], strides[0]),\n",
    "        residual_body(64, repetitions, strides[1:]),\n",
    "        head(64 * 2**(len(repetitions) - 1), classes, p_drop)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet([2, 2, 2, 2], NUM_CLASSES, strides=[1, 1, 2, 2, 2], p_drop=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Sequential(\n",
       "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): ReLU(inplace=True)\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (shortcut): Sequential()\n",
       "      (residual): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (act): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (shortcut): Sequential()\n",
       "      (residual): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (act): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): ResidualBlock(\n",
       "      (shortcut): Sequential(\n",
       "        (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        (1): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (residual): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (act): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): ResidualBlock(\n",
       "      (shortcut): Sequential()\n",
       "      (residual): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (act): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): ResidualBlock(\n",
       "      (shortcut): Sequential(\n",
       "        (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        (1): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (residual): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (act): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): ResidualBlock(\n",
       "      (shortcut): Sequential()\n",
       "      (residual): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (act): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): ResidualBlock(\n",
       "      (shortcut): Sequential(\n",
       "        (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        (1): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (residual): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (act): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): ResidualBlock(\n",
       "      (shortcut): Sequential()\n",
       "      (residual): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (act): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (2): Sequential(\n",
       "    (0): AdaptiveAvgPool2d(output_size=1)\n",
       "    (1): Flatten(start_dim=1, end_dim=-1)\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(init_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy\n",
    "\n",
    "class ClassificationTask(pl.LightningModule):\n",
    "    def __init__(self, model, max_lr, epochs, steps_per_epoch):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters('max_lr', 'epochs')\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.model = model\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.train_acc = Accuracy(compute_on_step=False)\n",
    "        self.val_acc = Accuracy(compute_on_step=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def _shared_step(self, batch, metric, prefix):\n",
    "        x, y = batch\n",
    "        logits = self.model(x)\n",
    "        loss = self.loss(logits, y)\n",
    "        metric(logits, y)\n",
    "        self.log(f'{prefix}_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._shared_step(batch, self.train_acc, 'train')\n",
    "    \n",
    "    def training_epoch_end(self, outs):\n",
    "        self.log('train_acc', self.train_acc.compute())\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self._shared_step(batch, self.val_acc, 'val')\n",
    "    \n",
    "    def validation_epoch_end(self, val_outs):\n",
    "        self.log('val_acc', self.val_acc.compute())\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.model.parameters(), weight_decay=1e-2)\n",
    "        lr_scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=self.hparams.max_lr,\n",
    "                                                     steps_per_epoch=self.steps_per_epoch,\n",
    "                                                     epochs=self.hparams.epochs)\n",
    "        lr_dict = {'scheduler': lr_scheduler, 'interval': 'step'}\n",
    "        return [optimizer], [lr_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = ClassificationTask(\n",
    "    model=model,\n",
    "    max_lr=1e-2,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=len(data_module.train_dataloader())\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "#trainer = pl.Trainer(gpus=1, max_epochs=EPOCHS+1)\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=EPOCHS+1,\n",
    "    logger=wandb_logger\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.9/multiprocessing/queues.py\", line 244, in _feed\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/usr/lib/python3.9/multiprocessing/queues.py\", line 244, in _feed\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/usr/lib/python3.9/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "  File \"/usr/lib/python3.9/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/multiprocessing/reductions.py\", line 319, in reduce_storage\n",
      "    metadata = storage._share_filename_()\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/multiprocessing/reductions.py\", line 319, in reduce_storage\n",
      "    metadata = storage._share_filename_()\n",
      "RuntimeError: falseINTERNAL ASSERT FAILED at \"../aten/src/ATen/MapAllocator.cpp\":300, please report a bug to PyTorch. unable to write to file </torch_23618_1>\n",
      "RuntimeError: falseINTERNAL ASSERT FAILED at \"../aten/src/ATen/MapAllocator.cpp\":300, please report a bug to PyTorch. unable to write to file </torch_23604_1>\n",
      "ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 23643) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:990\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 990\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    991\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n",
      "File \u001b[0;32m/usr/lib/python3.9/multiprocessing/queues.py:114\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll(timeout):\n\u001b[0;32m--> 114\u001b[0m         \u001b[39mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll():\n",
      "\u001b[0;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lr_finder \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtuner\u001b[38;5;241m.\u001b[39mlr_filr_finder \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtuner\u001b[38;5;241m.\u001b[39mlr_find(classifier, datamodule\u001b[38;5;241m=\u001b[39mdata_module, min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m, max_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/tuner/tuning.py:199\u001b[0m, in \u001b[0;36mTuner.lr_find\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, min_lr, max_lr, num_training, mode, early_stop_threshold, update_attr)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[39m\"\"\"Enables the user to do a range test of good initial learning rates, to reduce the amount of guesswork in\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[39mpicking a good starting learning rate.\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[39m        or if you are using more than one optimizer.\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mauto_lr_find \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mtune(\n\u001b[1;32m    200\u001b[0m     model,\n\u001b[1;32m    201\u001b[0m     train_dataloaders\u001b[39m=\u001b[39;49mtrain_dataloaders,\n\u001b[1;32m    202\u001b[0m     val_dataloaders\u001b[39m=\u001b[39;49mval_dataloaders,\n\u001b[1;32m    203\u001b[0m     datamodule\u001b[39m=\u001b[39;49mdatamodule,\n\u001b[1;32m    204\u001b[0m     lr_find_kwargs\u001b[39m=\u001b[39;49m{\n\u001b[1;32m    205\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mmin_lr\u001b[39;49m\u001b[39m\"\u001b[39;49m: min_lr,\n\u001b[1;32m    206\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mmax_lr\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_lr,\n\u001b[1;32m    207\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mnum_training\u001b[39;49m\u001b[39m\"\u001b[39;49m: num_training,\n\u001b[1;32m    208\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mmode\u001b[39;49m\u001b[39m\"\u001b[39;49m: mode,\n\u001b[1;32m    209\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mearly_stop_threshold\u001b[39;49m\u001b[39m\"\u001b[39;49m: early_stop_threshold,\n\u001b[1;32m    210\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mupdate_attr\u001b[39;49m\u001b[39m\"\u001b[39;49m: update_attr,\n\u001b[1;32m    211\u001b[0m     },\n\u001b[1;32m    212\u001b[0m )\n\u001b[1;32m    213\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mauto_lr_find \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[39mreturn\u001b[39;00m result[\u001b[39m\"\u001b[39m\u001b[39mlr_find\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py:1058\u001b[0m, in \u001b[0;36mTrainer.tune\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, scale_batch_size_kwargs, lr_find_kwargs)\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(\n\u001b[1;32m   1054\u001b[0m     model, train_dataloaders\u001b[39m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[39m=\u001b[39mval_dataloaders, datamodule\u001b[39m=\u001b[39mdatamodule\n\u001b[1;32m   1055\u001b[0m )\n\u001b[1;32m   1057\u001b[0m \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1058\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtuner\u001b[39m.\u001b[39;49m_tune(\n\u001b[1;32m   1059\u001b[0m         model, scale_batch_size_kwargs\u001b[39m=\u001b[39;49mscale_batch_size_kwargs, lr_find_kwargs\u001b[39m=\u001b[39;49mlr_find_kwargs\n\u001b[1;32m   1060\u001b[0m     )\n\u001b[1;32m   1062\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m   1063\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtuning \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/tuner/tuning.py:70\u001b[0m, in \u001b[0;36mTuner._tune\u001b[0;34m(self, model, scale_batch_size_kwargs, lr_find_kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mauto_lr_find:\n\u001b[1;32m     69\u001b[0m     lr_find_kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m\"\u001b[39m\u001b[39mupdate_attr\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 70\u001b[0m     result[\u001b[39m\"\u001b[39m\u001b[39mlr_find\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m lr_find(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer, model, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mlr_find_kwargs)\n\u001b[1;32m     72\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mFINISHED\n\u001b[1;32m     74\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/tuner/lr_finder.py:244\u001b[0m, in \u001b[0;36mlr_find\u001b[0;34m(trainer, model, min_lr, max_lr, num_training, mode, early_stop_threshold, update_attr)\u001b[0m\n\u001b[1;32m    241\u001b[0m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39msetup_optimizers \u001b[39m=\u001b[39m lr_finder\u001b[39m.\u001b[39m_exchange_scheduler(trainer, model)  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[39m# Fit, lr & loss logged in callback\u001b[39;00m\n\u001b[0;32m--> 244\u001b[0m trainer\u001b[39m.\u001b[39;49mtuner\u001b[39m.\u001b[39;49m_run(model)\n\u001b[1;32m    246\u001b[0m \u001b[39m# Prompt if we stopped early\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mglobal_step \u001b[39m!=\u001b[39m num_training:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/tuner/tuning.py:80\u001b[0m, in \u001b[0;36mTuner._run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mRUNNING  \u001b[39m# last `_run` call might have set it to `FINISHED`\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_run(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     81\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mtuning \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py:1166\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1164\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> 1166\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   1168\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1169\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py:1252\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   1251\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1252\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py:1274\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1271\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pre_training_routine()\n\u001b[1;32m   1273\u001b[0m \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1274\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check()\n\u001b[1;32m   1276\u001b[0m \u001b[39m# enable train mode\u001b[39;00m\n\u001b[1;32m   1277\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py:1342\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1340\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[1;32m   1341\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m-> 1342\u001b[0m     val_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1344\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1346\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/loop.py:200\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 200\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:155\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_dataloaders \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    154\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdataloader_idx\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m dataloader_idx\n\u001b[0;32m--> 155\u001b[0m dl_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher, dl_max_batches, kwargs)\n\u001b[1;32m    157\u001b[0m \u001b[39m# store batch level output per dataloader\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs\u001b[39m.\u001b[39mappend(dl_outputs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/loop.py:200\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 200\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:127\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.advance\u001b[0;34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data_fetcher, DataLoaderIterDataFetcher):\n\u001b[1;32m    126\u001b[0m     batch_idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mcurrent\u001b[39m.\u001b[39mready\n\u001b[0;32m--> 127\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(data_fetcher)\n\u001b[1;32m    128\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     batch_idx, batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(data_fetcher)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/fetching.py:185\u001b[0m, in \u001b[0;36mAbstractDataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m--> 185\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfetching_function()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/fetching.py:264\u001b[0m, in \u001b[0;36mDataFetcher.fetching_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone:\n\u001b[1;32m    262\u001b[0m     \u001b[39m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[1;32m    263\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 264\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fetch_next_batch(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataloader_iter)\n\u001b[1;32m    265\u001b[0m         \u001b[39m# consume the batch we just fetched\u001b[39;00m\n\u001b[1;32m    266\u001b[0m         batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatches\u001b[39m.\u001b[39mpop(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/fetching.py:278\u001b[0m, in \u001b[0;36mDataFetcher._fetch_next_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fetch_next_batch\u001b[39m(\u001b[39mself\u001b[39m, iterator: Iterator) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m     start_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_fetch_start()\n\u001b[0;32m--> 278\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(iterator)\n\u001b[1;32m    279\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfetched \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprefetch_batches \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_len:\n\u001b[1;32m    281\u001b[0m         \u001b[39m# when we don't prefetch but the dataloader is sized, we use the length for `done`\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    520\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> 521\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    522\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    523\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    524\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    525\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:1186\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1183\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[1;32m   1185\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1186\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[1;32m   1187\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[1;32m   1189\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:1152\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1148\u001b[0m     \u001b[39m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1149\u001b[0m     \u001b[39m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1150\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1151\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 1152\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[1;32m   1153\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[1;32m   1154\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:1003\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1001\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(failed_workers) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1002\u001b[0m     pids_str \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(w\u001b[39m.\u001b[39mpid) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m failed_workers)\n\u001b[0;32m-> 1003\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mDataLoader worker (pid(s) \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) exited unexpectedly\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(pids_str)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m   1004\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, queue\u001b[39m.\u001b[39mEmpty):\n\u001b[1;32m   1005\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mFalse\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 23643) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "lr_finder = trainer.tuner.lr_filr_finder = trainer.tuner.lr_find(classifier, datamodule=data_module, min_lr=1e-6, max_lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = lr_finder.plot(suggest=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | Sequential       | 11.2 M\n",
      "1 | loss      | CrossEntropyLoss | 0     \n",
      "2 | train_acc | Accuracy         | 0     \n",
      "3 | val_acc   | Accuracy         | 0     \n",
      "-----------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.804    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b10b2fc51c49b0bd2a4235174d7cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b4d869411454ff9928d052a108a1d24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "072d0af1e7ef4113981ca74ee0915230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "value cannot be converted to type float without overflow: -3.54813e+38",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(classifier, datamodule\u001b[38;5;241m=\u001b[39mdata_module)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py:700\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    682\u001b[0m \u001b[39mRuns the full optimization routine.\u001b[39;00m\n\u001b[1;32m    683\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[39m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    699\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 700\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    701\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    702\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py:654\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    652\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    653\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 654\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    655\u001b[0m \u001b[39m# TODO(awaelchli): Unify both exceptions below, where `KeyboardError` doesn't re-raise\u001b[39;00m\n\u001b[1;32m    656\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m exception:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py:741\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    737\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    738\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_ckpt_path(\n\u001b[1;32m    739\u001b[0m     ckpt_path, model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    740\u001b[0m )\n\u001b[0;32m--> 741\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    743\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    744\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py:1166\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1164\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> 1166\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   1168\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1169\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py:1252\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   1251\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1252\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py:1282\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1280\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[1;32m   1281\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1282\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/loop.py:200\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 200\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/fit_loop.py:269\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher\u001b[39m.\u001b[39msetup(\n\u001b[1;32m    266\u001b[0m     dataloader, batch_to_device\u001b[39m=\u001b[39mpartial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_call_strategy_hook, \u001b[39m\"\u001b[39m\u001b[39mbatch_to_device\u001b[39m\u001b[39m\"\u001b[39m, dataloader_idx\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    267\u001b[0m )\n\u001b[1;32m    268\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 269\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/loop.py:200\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 200\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:203\u001b[0m, in \u001b[0;36mTrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[1;32m    202\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_batch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 203\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_loop\u001b[39m.\u001b[39;49mrun(kwargs)\n\u001b[1;32m    205\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[1;32m    207\u001b[0m \u001b[39m# update non-plateau LR schedulers\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[39m# update epoch-interval ones only when we are at the end of training epoch\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/loop.py:200\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 200\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/batch/training_batch_loop.py:87\u001b[0m, in \u001b[0;36mTrainingBatchLoop.advance\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39mautomatic_optimization:\n\u001b[1;32m     84\u001b[0m     optimizers \u001b[39m=\u001b[39m _get_active_optimizers(\n\u001b[1;32m     85\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39moptimizers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39moptimizer_frequencies, kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mbatch_idx\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m0\u001b[39m)\n\u001b[1;32m     86\u001b[0m     )\n\u001b[0;32m---> 87\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer_loop\u001b[39m.\u001b[39;49mrun(optimizers, kwargs)\n\u001b[1;32m     88\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_loop\u001b[39m.\u001b[39mrun(kwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/loop.py:200\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 200\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:201\u001b[0m, in \u001b[0;36mOptimizerLoop.advance\u001b[0;34m(self, optimizers, kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madvance\u001b[39m(\u001b[39mself\u001b[39m, optimizers: List[Tuple[\u001b[39mint\u001b[39m, Optimizer]], kwargs: OrderedDict) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:  \u001b[39m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m    199\u001b[0m     kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_kwargs(kwargs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer_idx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hiddens)\n\u001b[0;32m--> 201\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_optimization(kwargs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizers[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptim_progress\u001b[39m.\u001b[39;49moptimizer_position])\n\u001b[1;32m    202\u001b[0m     \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mloss \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m         \u001b[39m# automatic optimization assumes a loss needs to be returned for extras to be considered as the batch\u001b[39;00m\n\u001b[1;32m    204\u001b[0m         \u001b[39m# would be skipped otherwise\u001b[39;00m\n\u001b[1;32m    205\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer_idx] \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39masdict()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:248\u001b[0m, in \u001b[0;36mOptimizerLoop._run_optimization\u001b[0;34m(self, kwargs, optimizer)\u001b[0m\n\u001b[1;32m    240\u001b[0m         closure()\n\u001b[1;32m    242\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[39m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[39m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    247\u001b[0m     \u001b[39m# the `batch_idx` is optional with inter-batch parallelism\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer_step(optimizer, opt_idx, kwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mbatch_idx\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m0\u001b[39;49m), closure)\n\u001b[1;32m    250\u001b[0m result \u001b[39m=\u001b[39m closure\u001b[39m.\u001b[39mconsume_result()\n\u001b[1;32m    252\u001b[0m \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mloss \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     \u001b[39m# if no result, user decided to skip optimization\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     \u001b[39m# otherwise update running loss + reset accumulated loss\u001b[39;00m\n\u001b[1;32m    255\u001b[0m     \u001b[39m# TODO: find proper way to handle updating running loss\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:358\u001b[0m, in \u001b[0;36mOptimizerLoop._optimizer_step\u001b[0;34m(self, optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_ready()\n\u001b[1;32m    357\u001b[0m \u001b[39m# model hook\u001b[39;00m\n\u001b[0;32m--> 358\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_lightning_module_hook(\n\u001b[1;32m    359\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39moptimizer_step\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    360\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mcurrent_epoch,\n\u001b[1;32m    361\u001b[0m     batch_idx,\n\u001b[1;32m    362\u001b[0m     optimizer,\n\u001b[1;32m    363\u001b[0m     opt_idx,\n\u001b[1;32m    364\u001b[0m     train_step_and_backward_closure,\n\u001b[1;32m    365\u001b[0m     on_tpu\u001b[39m=\u001b[39;49m\u001b[39misinstance\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49maccelerator, TPUAccelerator),\n\u001b[1;32m    366\u001b[0m     using_native_amp\u001b[39m=\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mamp_backend \u001b[39m==\u001b[39;49m AMPType\u001b[39m.\u001b[39;49mNATIVE),\n\u001b[1;32m    367\u001b[0m     using_lbfgs\u001b[39m=\u001b[39;49mis_lbfgs,\n\u001b[1;32m    368\u001b[0m )\n\u001b[1;32m    370\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m should_accumulate:\n\u001b[1;32m    371\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_completed()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py:1549\u001b[0m, in \u001b[0;36mTrainer._call_lightning_module_hook\u001b[0;34m(self, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1546\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m hook_name\n\u001b[1;32m   1548\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningModule]\u001b[39m\u001b[39m{\u001b[39;00mpl_module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1549\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1551\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/core/module.py:1666\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs)\u001b[0m\n\u001b[1;32m   1584\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimizer_step\u001b[39m(\n\u001b[1;32m   1585\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1586\u001b[0m     epoch: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1593\u001b[0m     using_lbfgs: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1594\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1595\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1596\u001b[0m \u001b[39m    Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1597\u001b[0m \u001b[39m    each optimizer.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1664\u001b[0m \n\u001b[1;32m   1665\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1666\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49moptimizer_closure)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/core/optimizer.py:168\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[39mraise\u001b[39;00m MisconfigurationException(\u001b[39m\"\u001b[39m\u001b[39mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    167\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strategy \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_strategy\u001b[39m.\u001b[39;49moptimizer_step(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer_idx, closure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    170\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_on_after_step()\n\u001b[1;32m    172\u001b[0m \u001b[39mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/strategies/strategy.py:216\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, opt_idx, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39m\"\"\"Performs the actual optimizer step.\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \n\u001b[1;32m    208\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39m    **kwargs: Any extra arguments to ``optimizer.step``\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    215\u001b[0m model \u001b[39m=\u001b[39m model \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module\n\u001b[0;32m--> 216\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprecision_plugin\u001b[39m.\u001b[39;49moptimizer_step(model, optimizer, opt_idx, closure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/plugins/precision/precision_plugin.py:153\u001b[0m, in \u001b[0;36mPrecisionPlugin.optimizer_step\u001b[0;34m(self, model, optimizer, optimizer_idx, closure, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(model, pl\u001b[39m.\u001b[39mLightningModule):\n\u001b[1;32m    152\u001b[0m     closure \u001b[39m=\u001b[39m partial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_closure, model, optimizer, optimizer_idx, closure)\n\u001b[0;32m--> 153\u001b[0m \u001b[39mreturn\u001b[39;00m optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49mclosure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/optim/lr_scheduler.py:65\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     64\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/optim/adamw.py:137\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[39m# record the step after step update\u001b[39;00m\n\u001b[1;32m    135\u001b[0m         state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 137\u001b[0m     F\u001b[39m.\u001b[39;49madamw(params_with_grad,\n\u001b[1;32m    138\u001b[0m             grads,\n\u001b[1;32m    139\u001b[0m             exp_avgs,\n\u001b[1;32m    140\u001b[0m             exp_avg_sqs,\n\u001b[1;32m    141\u001b[0m             max_exp_avg_sqs,\n\u001b[1;32m    142\u001b[0m             state_steps,\n\u001b[1;32m    143\u001b[0m             amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    144\u001b[0m             beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    145\u001b[0m             beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    146\u001b[0m             lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    147\u001b[0m             weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    148\u001b[0m             eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    150\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/optim/_functional.py:143\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m    139\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39msqrt(bias_correction2))\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    141\u001b[0m step_size \u001b[39m=\u001b[39m lr \u001b[39m/\u001b[39m bias_correction1\n\u001b[0;32m--> 143\u001b[0m param\u001b[39m.\u001b[39;49maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49mstep_size)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: value cannot be converted to type float without overflow: -3.54813e+38"
     ]
    }
   ],
   "source": [
    "trainer.fit(classifier, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint('model_save.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
